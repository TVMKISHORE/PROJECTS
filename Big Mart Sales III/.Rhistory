#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
y_pred<- predict(GBM,sparse_matrix_test)
test_result =as.data.frame(test[c("Item_Identifier","Outlet_Identifier")])
y_pred<- as.data.frame(y_pred)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.0029,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 4,    #.76388
min_child_weight=4,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
y_pred<- predict(GBM,sparse_matrix_test)
test_result =as.data.frame(test[c("Item_Identifier","Outlet_Identifier")])
y_pred<- as.data.frame(y_pred)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.0032,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 4,    #.76388
min_child_weight=4,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
y_pred<- predict(GBM,sparse_matrix_test)
test_result =as.data.frame(test[c("Item_Identifier","Outlet_Identifier")])
y_pred<- as.data.frame(y_pred)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.003,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 4,    #.76388
min_child_weight=4,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3525
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
y_pred<- predict(GBM,sparse_matrix_test)
test_result =as.data.frame(test[c("Item_Identifier","Outlet_Identifier")])
y_pred<- as.data.frame(y_pred)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.003,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 3,    #.76388
min_child_weight=4,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
y_pred<- predict(GBM,sparse_matrix_test)
test_result =as.data.frame(test[c("Item_Identifier","Outlet_Identifier")])
y_pred<- as.data.frame(y_pred)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.003,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 5,    #.76388
min_child_weight=4,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.003,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 4,    #.76388
min_child_weight=10,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
y_pred<- as.data.frame(y_pred)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.003,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 4,    #.76388
min_child_weight=4,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
y_pred<- predict(GBM,sparse_matrix_test)
test_result =as.data.frame(test[c("Item_Identifier","Outlet_Identifier")])
y_pred<- as.data.frame(y_pred)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
train1=train
train1["Item_Weight"]=log(train1["Item_Weight"])
train1["Item_Fat_Content"]=log(train1["Item_Fat_Content"])
str(train1)
train1["Item_Visibility"]=log(train1["Item_Visibility"])
train1["Item_MRP"]=log(train1["Item_MRP"])
train1["outlet_age"]=log(train1["outlet_age"])
test1=test
test1["Item_Weight"]=log(test1["Item_Weight"])
test1["Item_Visibility"]=log(test1["Item_Visibility"])
test1["Item_MRP"]=log(test1["Item_MRP"])
test1["outlet_age"]=log(test1["outlet_age"])
X=train1[cols]
sparse_matrix<-sparse.model.matrix(  ~ .-1, data=X)
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.003,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 4,    #.76388
min_child_weight=4,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
X_test=test1[cols]
sparse_matrix_test<-sparse.model.matrix( ~ .-1, data=X_test)
y_pred<- predict(GBM,sparse_matrix_test)
test_result =as.data.frame(test[c("Item_Identifier","Outlet_Identifier")])
y_pred<- as.data.frame(y_pred)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
log2
log(2)
e(log(2))
exp(log(2))
output_vector <- log(train[,"Item_Outlet_Sales"])
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.003,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 4,    #.76388
min_child_weight=4,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
y_pred<- exp(predict(GBM,sparse_matrix_test))
test_result =as.data.frame(test[c("Item_Identifier","Outlet_Identifier")])
y_pred<- as.data.frame(y_pred)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
View(importance)
View(importance)
X=train[cols]
sparse_matrix<-sparse.model.matrix(  ~ .-1, data=X)
sparse_matrix=as.matrix(sparse_matrix)
sparse_matrix=as.data.frame(sparse_matrix)
View(sparse_matrix)
View(sparse_matrix)
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.003,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 4,    #.76388
min_child_weight=4,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
names(sparse_matrix)
View(importance)
View(importance)
View(importance)
View(importance)
cols=
("Item_MRP",
"Outlet_TypeSupermarket Type1",
"Outlet_IdentifierOUT027",
"outlet_age",
"Item_Visibility",
"Outlet_TypeSupermarket Type3",
"Item_Weight",
"Outlet_IdentifierOUT019",
"Outlet_SizeSmall",
"Outlet_IdentifierOUT018",
"Outlet_Location_TypeTier 3",
"Outlet_SizeMedium")
cols=c("Item_MRP", "Outlet_TypeSupermarket Type1", "Outlet_IdentifierOUT027",
"outlet_age","Item_Visibility","Outlet_TypeSupermarket Type3",
"Item_Weight","Outlet_IdentifierOUT019","Outlet_SizeSmall",
"Outlet_IdentifierOUT018","Outlet_Location_TypeTier 3","Outlet_SizeMedium")
sparse_matrix<-sparse.model.matrix(  ~ .-1, data=X)
temp=as.matrix(sparse_matrix)
temp=as.data.frame(temp)
cols=c("Item_MRP", "Outlet_TypeSupermarket Type1", "Outlet_IdentifierOUT027",
"outlet_age","Item_Visibility","Outlet_TypeSupermarket Type3",
"Item_Weight","Outlet_IdentifierOUT019","Outlet_SizeSmall",
"Outlet_IdentifierOUT018","Outlet_Location_TypeTier 3","Outlet_SizeMedium")
X=temp[cols]
sparse_matrix<-sparse.model.matrix(  ~ .-1, data=X)
View(X)
View(X)
sparse_matrix<-sparse.model.matrix(  ~ .-1, data=X)
X=as.matrix(temp[cols])
sparse_matrix<-sparse.model.matrix(  ~ .-1, data=X)
sparse_matrix=X
output_vector <- log(train[,"Item_Outlet_Sales"])
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.003,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 4,    #.76388
min_child_weight=4,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
View(X)
View(X)
y_pred<- exp(predict(GBM,sparse_matrix_test))
test_result =as.data.frame(test[c("Item_Identifier","Outlet_Identifier")])
y_pred<- as.data.frame(y_pred)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
output_vector <- train[,"Item_Outlet_Sales"]
GBM <- xgboost(data = sparse_matrix,
label = output_vector,
eta = 0.003,                      # Learning rate
#max_depth = 4,   #.75
#max_depth = 3,   #.756
#max_depth = 2,   #.7569
max_depth = 4,    #.76388
min_child_weight=4,  #
colsample_bytree=.5,  #Tatio of columns when constructing tree Default is :1
subsample = 0.60,
set.seed=123,
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
#objective = "binary:logitraw",  #logistic regression for binary classification, outputscore before logistic transformation
#objective = "binary:logistic",   # logistic regression for binary classification. Output probability.
objective = "reg:linear",  #Linear Regression
nthread = 4,
#lambda=.1,     #L2 regularization
#alpha=.1000     #L1 regularization
#num_parallel_tree=100,
nround=3500
#early.stop.round=10  # same result observed in 'n' rounds, then stops execution
)
View(X_test)
View(X_test)
X_test=test[cols]
cols=c( "Item_Weight" ,   "Item_Fat_Content",
"Item_Visibility",       "Item_Type" ,     "Item_MRP"  ,
"Outlet_Identifier",     "outlet_age",     "Outlet_Size"     ,
"Outlet_Location_Type",  "Outlet_Type"   )
#       "Outlet_Location_Type",      "Outlet_Type" , "Outlet_Weight"      )
X_test=test[cols]
sparse_matrix_test<-sparse.model.matrix( ~ .-1, data=X_test)
temp=as.matrix(sparse_matrix_test)
temp=as.data.frame(temp)
cols=c("Item_MRP", "Outlet_TypeSupermarket Type1", "Outlet_IdentifierOUT027",
"outlet_age","Item_Visibility","Outlet_TypeSupermarket Type3",
"Item_Weight","Outlet_IdentifierOUT019","Outlet_SizeSmall",
"Outlet_IdentifierOUT018","Outlet_Location_TypeTier 3","Outlet_SizeMedium")
X_test=as.matrix(temp[cols])
sparse_matrix<-sparse.model.matrix(  ~ .-1, data=X_test)
X_test=as.matrix(temp[cols])
sparse_matrix_test=X_test
y_pred<- predict(GBM,sparse_matrix_test)
test_result =as.data.frame(test[c("Item_Identifier","Outlet_Identifier")])
y_pred<- as.data.frame(y_pred)
test_result['Item_Outlet_Sales'] = y_pred
Sales_file=test_result
row.names(Sales_file)<-NULL
#Loan_file$Loan_Status[Loan_file$Loan_Status>=.5] <- "Y"
#Loan_file$Loan_Status[Loan_file$Loan_Status<.5] <- "N"
write.csv(Sales_file,file='Sales_Result_GBM.csv')
View(temp)
View(temp)
temp[!cols]
temp[!cols_init]
temp[!(cols_init)]
temp[cols_init]
cols_init
cols_init=c( "Item_Weight" ,   "Item_Fat_Content",
"Item_Visibility",       "Item_Type" ,     "Item_MRP"  ,
"Outlet_Identifier",     "outlet_age",     "Outlet_Size"     ,
"Outlet_Location_Type",  "Outlet_Type"   )
#       "Outlet_Location_Type",      "Outlet_Type" , "Outlet_Weight"      )
cols_init
temp[cols_init]
temp[(cols_init)]
temp['cols_init']
temp[cols_init]
cols=c("Item_MRP", "Outlet_TypeSupermarket Type1", "Outlet_IdentifierOUT027",
"outlet_age","Item_Visibility","Outlet_TypeSupermarket Type3",
"Item_Weight","Outlet_IdentifierOUT019","Outlet_SizeSmall",
"Outlet_IdentifierOUT018","Outlet_Location_TypeTier 3","Outlet_SizeMedium")
temp[!cols]
temp[cols]
temp[-cols]
temp[-(cols)]
temp[- cols]
temp[ , !cols]
temp[ , (cols):=NULL]
temp[ , (cols) :=NULL]
temp[ , cols]
temp[ , cols]<-NULL
View(temp)
View(temp)
set.seed(1)
KMC = kmeans(temp, centers = 5, iter.max = 1000)
str(KMC)
KMC$centers[2]
KMC$centers[1]
KMC$centers[3]
KMC$centers[4]
KMC$centers[5]
KMC$centers
str(KMC)
KMC$cluster
str(KMC)
aggregate(temp,by=list(MKC$cluster),FUN=mean)
aggregate(temp,by=list(KMC$cluster),FUN=mean)
test=aggregate(temp,by=list(KMC$cluster),FUN=mean)
View(test)
View(test)
